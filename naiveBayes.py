# -*- coding: utf-8 -*-
"""DM Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uNRckh99blVvWK9tYJ6nVx1j9jGC2iwq

**Author:** Nushra Zannat

**Algorithm:** Naive Bayes

### University of Oklahoma
---

# Importing Dataset
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import preprocessing

df = pd.read_csv("/content/datasets/train_strokes.csv")

df.head()

"""# Data Cleaning"""

df.columns = df.columns.str.lower()

"""### Dropping duplicate rows

"""

#removing any duplicate rows, directly in the original df
df.drop_duplicates(inplace=True) 

df.shape

"""### Handling Missing Values"""

df.isna().sum() 
# contains NA: bmi, smoking_status

df.isna().sum()/df.shape[0] * 100

# ~30% of the data in the smoking_status is missing. 3.3% in bmi

df.bmi.fillna(df.bmi.mean(), inplace=True) #filling bmi with mean

#df.smoking_status.fillna(method = 'ffill') 
df.smoking_status.fillna("never smoked", inplace = True) 
df.smoking_status
# should we consider 'unknown' to be missing value? 
# if so, the percentage of NA in this column goes up
# what method to use? ffill or mode?

"""# DATA Pre-Processing (Naive Bayes)"""

df1 = df
# encoding categorical variables
for i in ['gender','ever_married','work_type','residence_type','smoking_status']:
  df1[i] = preprocessing.LabelEncoder().fit_transform(df1[i])

df1.head()

corr = df1.corr(method ='pearson')
sns.heatmap(corr,vmax=1, vmin=-0.4, square=True, linewidths=0.3, cmap='Purples')

#very low correlation among the features, which is good, as naive bayes assumes features to be independent of each other. Don't have to pre-process any further

# some functions I'll need
def meanDefined(values):
  return sum(values)/float(len(values))

def st_dev(values):
  mean = meanDefined(values)
  dev = [(x - mean)**2 for x in values] # xi - mean (diff of each individual value from the mean)
  var = sum(dev)/float(len(values))
  return np.sqrt(var)

class NBayes:
  def train(self,X,y):
    #storing the dimenstions in seperate vars as n(obs) and n(features) for later use
    self.n_obs, self.n_features = X.shape
  
    self.classes = list(np.unique(y)) # [0,1]
    self.n_c = len(self.classes) #2

    self.mean = np.empty((self.n_c, self.n_features))
    self.sdev = np.empty((self.n_c, self.n_features))
    self.prior = []

    for i in range(self.n_c):
      df = X[y==self.classes[i]]
      self.mean[i,:] = np.mean(df, axis=0)
      self.sdev[i,:] = np.var(df, axis=0)
      #prior calculation
      self.prior.append(len(X[y==i])/len(y))

  def predict(self, X):
        y_pred = [self.posterior_prob(x) for x in X]
        return np.array(y_pred)
  
  def posterior_prob(self,x):
    posteriors = list()

    for ind,i in enumerate(self.classes):
      likelihood = np.sum(np.log(self.calculate_gaussian_density(x,ind)))
      posterior = likelihood + np.log(self.prior[ind])
      posteriors.append(posterior)

    # returning class with MAX posterior
    return self.classes[np.argmax(posteriors)]
  
  def calculate_gaussian_density(self,x,index):
    mean = self.mean[index]
    sdev = self.sdev[index]

    c = 1 / sdev * np.sqrt(2 * np.pi)
    p = np.exp(-((x-mean)**2/(2 * sdev**2)))

    return c*p

"""### Train-Test Split"""

X = df1.iloc[:,:-1]
del X[X.columns[0]]
Y = df1["stroke"]

n = X.shape[0]
n_train = int(0.7 * n)
n_test  = int(0.3 * n)

X_train = X[:n_train]
y_train = Y[:n_train]

X_test = X[n_train:]
y_test = Y[n_train:]

"""### trying something different to balance the target var"""

df2 = df1[df1['stroke'] == 1]
df3 = df1.sample(frac=0.03)

df4 = pd.concat([df2, df3],ignore_index=True)
df4.shape

df4.drop_duplicates(inplace=True)

df4.shape

df4["stroke"].hist()

Y

"""### Evaluate"""

nb = NBayes()

nb.train(X_train,y_train)

pred = nb.predict(y_test)

from sklearn.metrics import confusion_matrix,f1_score, accuracy_score

confusion_matrix(y_test,pred)

accuracy_score(y_test,pred)